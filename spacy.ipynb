{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pymupdf\n",
    "#!pip install --upgrade pytesseract\n",
    "#!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./sample.pdf\"\n",
    "output_dir = \"./output\"\n",
    "spacy_model = \"en_core_web_md\"\n",
    "tesseract_path = \"C:/Program Files/Tesseract-OCR/tesseract.exe\" # Default installation path of Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "from pathlib import Path\n",
    "debug_dir = f\"{output_dir}/debug\"\n",
    "Path(debug_dir).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pdf to image\n",
    "PDFs can have incorrect text encoding or contain images withing itself, it is better to \n",
    "convert pdf pages to images and then perform OCR to extract the text.\n",
    "\n",
    "We convert only the first page to an image since we assume that it's the title page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "dpi = 300\n",
    "zoom_factor = dpi / 72\n",
    "\n",
    "# Read first page\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc[0]\n",
    "\n",
    "# Convert to image\n",
    "mat = fitz.Matrix(zoom_factor, zoom_factor)\n",
    "pix = page.get_pixmap(matrix=mat)\n",
    "image_data = pix.tobytes(\"png\")\n",
    "image = Image.open(BytesIO(image_data))\n",
    "image.save(f\"{debug_dir}/first_page.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(spacy_model)\n",
    "except IOError as e:\n",
    "    if str(e).startswith(\"[E050] Can't find model\"):\n",
    "        !python -m spacy download {spacy_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare OCR\n",
    "\n",
    "https://github.com/UB-Mannheim/tesseract/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_path\n",
    "ocr_result = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features JSON saved to: ./output/result.json\n",
      "Annotated image saved to: ./output/result.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageDraw\n",
    "import json\n",
    "\n",
    "result_filename = \"result\"\n",
    "\n",
    "# Process the OCR data with spaCy to identify entities\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "features = {}\n",
    "for i, word in enumerate(ocr_result[\"text\"]):\n",
    "    if word.strip():  # Skip empty strings\n",
    "        x, y, w, h = ocr_result[\"left\"][i], ocr_result[\"top\"][i], ocr_result[\"width\"][i], ocr_result[\"height\"][i]\n",
    "        bbox = (x, y, x + w, y + h)\n",
    "        # Process each word with spaCy\n",
    "        spacy_doc = nlp(word)\n",
    "        for ent in spacy_doc.ents:\n",
    "            # Check if the entity is what we're looking for\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "                features.setdefault(\"authors\", []).append({\"text\": ent.text, \"bounding_box\": bbox})\n",
    "                draw.rectangle(bbox, outline=\"blue\", width=2)  # Draw rectangle for authors\n",
    "            elif ent.label_ == 'DATE':\n",
    "                features.setdefault(\"dates\", []).append({\"text\": ent.text, \"bounding_box\": bbox})\n",
    "                draw.rectangle(bbox, outline=\"red\", width=2)  # Draw rectangle for dates\n",
    "            # Add other conditions based on your requirements\n",
    "\n",
    "output_path = f\"{output_dir}/{result_filename}\"\n",
    "\n",
    "# Save the features and their bounding boxes to a JSON file\n",
    "with open(f\"{output_path}.json\", 'w') as json_file:\n",
    "    json.dump(features, json_file)\n",
    "\n",
    "# Save the annotated image\n",
    "image.save(f\"{output_path}.png\")\n",
    "\n",
    "print(f\"Extracted features JSON saved to: {output_path}.json\")\n",
    "print(f\"Annotated image saved to: {output_path}.png\")\n",
    "\n",
    "# Close the document\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# import pytesseract\n",
    "# from PIL import Image, ImageDraw\n",
    "# import io\n",
    "# import json\n",
    "\n",
    "# # Load a spaCy model for Named Entity Recognition\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Define the path to the PDF\n",
    "# pdf_path = './sample.pdf'\n",
    "\n",
    "# # Open the PDF with PyMuPDF\n",
    "# doc = fitz.open(pdf_path)\n",
    "# page = doc[0]\n",
    "\n",
    "# # Set the zoom factor for rendering the page\n",
    "# zoom_factor = 2  # Increase this factor for a higher resolution image\n",
    "# mat = fitz.Matrix(zoom_factor, zoom_factor)\n",
    "\n",
    "# # Convert the PDF page to a high-resolution image\n",
    "# pix = page.get_pixmap(matrix=mat)\n",
    "# image_bytes = pix.tobytes(\"png\")\n",
    "# image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "# # Use PyTesseract to do OCR on the image\n",
    "# ocr_result = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "# # Prepare to draw rectangles\n",
    "# draw = ImageDraw.Draw(image)\n",
    "\n",
    "# # Function to calculate the bounding box for an entity\n",
    "# def calculate_entity_bbox(entity, words):\n",
    "#     # Find the words that make up the entity\n",
    "#     entity_words = [word for word in words if entity.start_char >= word['start_char'] and entity.end_char <= word['end_char']]\n",
    "#     if not entity_words:\n",
    "#         return None\n",
    "\n",
    "#     # Calculate the bounding box for the entity\n",
    "#     x_min = min(word['bbox'][0] for word in entity_words)\n",
    "#     y_min = min(word['bbox'][1] for word in entity_words)\n",
    "#     x_max = max(word['bbox'][2] for word in entity_words)\n",
    "#     y_max = max(word['bbox'][3] for word in entity_words)\n",
    "#     return (x_min, y_min, x_max, y_max)\n",
    "\n",
    "# # Extract words along with their bounding boxes\n",
    "# words = []\n",
    "# features = {}\n",
    "# current_block_num = -1\n",
    "# current_line_num = -1\n",
    "# current_block_text = \"\"\n",
    "# current_block_words = []\n",
    "# for i, text in enumerate(ocr_result['text']):\n",
    "#     if text.strip():  # Non-empty string\n",
    "#         if ocr_result['block_num'][i] != current_block_num or ocr_result['line_num'][i] != current_line_num:\n",
    "#             # New block or line\n",
    "#             if current_block_words:\n",
    "#                 # Process the previous block of text\n",
    "#                 doc = nlp(current_block_text)\n",
    "#                 for entity in doc.ents:\n",
    "#                     bbox = calculate_entity_bbox(entity, current_block_words)\n",
    "#                     if bbox:\n",
    "#                         draw.rectangle(bbox, outline=\"green\", width=2)\n",
    "#                         features.setdefault(entity.label_, []).append({'text': entity.text, 'bounding_box': bbox})\n",
    "#             # Reset for the new block or line\n",
    "#             current_block_num = ocr_result['block_num'][i]\n",
    "#             current_line_num = ocr_result['line_num'][i]\n",
    "#             current_block_text = \"\"\n",
    "#             current_block_words = []\n",
    "\n",
    "#         # Append the word to the current block's text\n",
    "#         current_block_text += \" \" + text if current_block_text else text\n",
    "#         # Store the word's bounding box and its position in the block's text\n",
    "#         word_bbox = (ocr_result['left'][i], ocr_result['top'][i], ocr_result['width'][i] + ocr_result['left'][i], ocr_result['height'][i] + ocr_result['top'][i])\n",
    "#         current_block_words.append({'text': text, 'bbox': word_bbox, 'start_char': len(current_block_text) - len(text), 'end_char': len(current_block_text)})\n",
    "\n",
    "# # Don't forget to process the last block of text\n",
    "# if current_block_words:\n",
    "#     doc = nlp(current_block_text)\n",
    "#     for entity in doc.ents:\n",
    "#         bbox = calculate_entity_bbox(entity, current_block_words)\n",
    "#         if bbox:\n",
    "#             draw.rectangle(bbox, outline=\"green\", width=2)\n",
    "#             features.setdefault(entity.label_, []).append({'text': entity.text, 'bounding_box': bbox})\n",
    "\n",
    "# # Save the annotated image\n",
    "# annotated_image_path = './output/result.png'\n",
    "# image.save(annotated_image_path)\n",
    "\n",
    "# # Save the extracted features to a JSON file\n",
    "# json_path = './output/result.json'\n",
    "# with open(json_path, 'w') as json_file:\n",
    "#     json.dump(features, json_file)\n",
    "\n",
    "# # Print out the paths to the JSON and image files\n",
    "# print(f\"Extracted features JSON saved to: {json_path}\")\n",
    "# print(f\"Annotated image saved to: {annotated_image_path}\")\n",
    "\n",
    "# # Close the document\n",
    "# doc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
